{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36ee0b3",
   "metadata": {},
   "source": [
    "1:16 \n",
    "[Link](https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training/implementation-removing-punctuation?u=74412268)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bace280",
   "metadata": {},
   "source": [
    "Steps to NLP for ML:\n",
    "\n",
    "1) Raw text - mdoel cannot read text\n",
    "\n",
    "2) Tokenizing - telling model what to read from file \n",
    "\n",
    "3) Clean text - remove stop words, punctuation, stemming, etc.\n",
    "\n",
    "4) Vectorize - turn data to numeric form\n",
    "\n",
    "5) Machine Learning - put into model for train/test\n",
    "    - classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50544059",
   "metadata": {},
   "source": [
    "## 1) Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903e3db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive/Negative rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I think food should have flavor and texture and both were lacking.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Overall I was not impressed and would not go back.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>The whole experience was underwhelming, and I think we'll just go to Ninja Sushi next time.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing ou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   Review  \\\n",
       "996                                    I think food should have flavor and texture and both were lacking.   \n",
       "997                                                                              Appetite instantly gone.   \n",
       "998                                                    Overall I was not impressed and would not go back.   \n",
       "999           The whole experience was underwhelming, and I think we'll just go to Ninja Sushi next time.   \n",
       "1000  Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing ou...   \n",
       "\n",
       "     Positive/Negative rating  \n",
       "996                         0  \n",
       "997                         0  \n",
       "998                         0  \n",
       "999                         0  \n",
       "1000                        0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 100) #set layout table\n",
    "\n",
    "my_data = pd.read_csv(\"Restaurant_Reviews.tsv\", sep=\"\\t\", header=None)\n",
    "my_data.columns = [\"Review\",\"Positive/Negative rating\"]\n",
    "\n",
    "my_data[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf9a90",
   "metadata": {},
   "source": [
    "## 2) Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efad9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c58d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    # Use regular expression to remove punctuation and replace with a space\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e0e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'Message' column and create a new column 'Cleaned_Message'\n",
    "my_data['Cleaned_Review'] = my_data['Review'].apply(remove_punctuation)\n",
    "\n",
    "# Save the cleaned data to a new TSV file\n",
    "my_data.to_csv('my_datacleaned_file.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d792ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive/Negative rating</th>\n",
       "      <th>Cleaned_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I think food should have flavor and texture and both were lacking.</td>\n",
       "      <td>0</td>\n",
       "      <td>I think food should have flavor and texture and both were lacking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "      <td>Appetite instantly gone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Overall I was not impressed and would not go back.</td>\n",
       "      <td>0</td>\n",
       "      <td>Overall I was not impressed and would not go back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>The whole experience was underwhelming, and I think we'll just go to Ninja Sushi next time.</td>\n",
       "      <td>0</td>\n",
       "      <td>The whole experience was underwhelming  and I think we ll just go to Ninja Sushi next time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>Then  as if I hadn t wasted enough of my life there  they poured salt in the wound by drawing ou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   Review  \\\n",
       "996                                    I think food should have flavor and texture and both were lacking.   \n",
       "997                                                                              Appetite instantly gone.   \n",
       "998                                                    Overall I was not impressed and would not go back.   \n",
       "999           The whole experience was underwhelming, and I think we'll just go to Ninja Sushi next time.   \n",
       "1000  Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing ou...   \n",
       "\n",
       "     Positive/Negative rating  \\\n",
       "996                         0   \n",
       "997                         0   \n",
       "998                         0   \n",
       "999                         0   \n",
       "1000                        0   \n",
       "\n",
       "                                                                                           Cleaned_Review  \n",
       "996                                    I think food should have flavor and texture and both were lacking   \n",
       "997                                                                              Appetite instantly gone   \n",
       "998                                                    Overall I was not impressed and would not go back   \n",
       "999           The whole experience was underwhelming  and I think we ll just go to Ninja Sushi next time   \n",
       "1000  Then  as if I hadn t wasted enough of my life there  they poured salt in the wound by drawing ou...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clean_data = pd.read_csv(\"my_datacleaned_file.tsv\", sep=\"\\t\")\n",
    "\n",
    "my_clean_data[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307f381",
   "metadata": {},
   "source": [
    "## 3) Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1707f6",
   "metadata": {},
   "source": [
    "Chops up words in the cleaned_review column. Good for adding vocabulary to model's bank and preprocessing of smaller managable pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9feddded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive/Negative rating</th>\n",
       "      <th>Cleaned_Review</th>\n",
       "      <th>Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I think food should have flavor and texture and both were lacking.</td>\n",
       "      <td>0</td>\n",
       "      <td>I think food should have flavor and texture and both were lacking</td>\n",
       "      <td>[i, think, food, should, have, flavor, and, texture, and, both, were, lacking, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "      <td>Appetite instantly gone</td>\n",
       "      <td>[appetite, instantly, gone, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Overall I was not impressed and would not go back.</td>\n",
       "      <td>0</td>\n",
       "      <td>Overall I was not impressed and would not go back</td>\n",
       "      <td>[overall, i, was, not, impressed, and, would, not, go, back, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>The whole experience was underwhelming, and I think we'll just go to Ninja Sushi next time.</td>\n",
       "      <td>0</td>\n",
       "      <td>The whole experience was underwhelming  and I think we ll just go to Ninja Sushi next time</td>\n",
       "      <td>[the, whole, experience, was, underwhelming, and, i, think, we, ll, just, go, to, ninja, sushi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>Then  as if I hadn t wasted enough of my life there  they poured salt in the wound by drawing ou...</td>\n",
       "      <td>[then, as, if, i, hadn, t, wasted, enough, of, my, life, there, they, poured, salt, in, the, wou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   Review  \\\n",
       "996                                    I think food should have flavor and texture and both were lacking.   \n",
       "997                                                                              Appetite instantly gone.   \n",
       "998                                                    Overall I was not impressed and would not go back.   \n",
       "999           The whole experience was underwhelming, and I think we'll just go to Ninja Sushi next time.   \n",
       "1000  Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing ou...   \n",
       "\n",
       "     Positive/Negative rating  \\\n",
       "996                         0   \n",
       "997                         0   \n",
       "998                         0   \n",
       "999                         0   \n",
       "1000                        0   \n",
       "\n",
       "                                                                                           Cleaned_Review  \\\n",
       "996                                    I think food should have flavor and texture and both were lacking    \n",
       "997                                                                              Appetite instantly gone    \n",
       "998                                                    Overall I was not impressed and would not go back    \n",
       "999           The whole experience was underwhelming  and I think we ll just go to Ninja Sushi next time    \n",
       "1000  Then  as if I hadn t wasted enough of my life there  they poured salt in the wound by drawing ou...   \n",
       "\n",
       "                                                                                                Tokenized  \n",
       "996                      [i, think, food, should, have, flavor, and, texture, and, both, were, lacking, ]  \n",
       "997                                                                         [appetite, instantly, gone, ]  \n",
       "998                                        [overall, i, was, not, impressed, and, would, not, go, back, ]  \n",
       "999   [the, whole, experience, was, underwhelming, and, i, think, we, ll, just, go, to, ninja, sushi, ...  \n",
       "1000  [then, as, if, i, hadn, t, wasted, enough, of, my, life, there, they, poured, salt, in, the, wou...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uses re\n",
    "\n",
    "def data_tokenization(text):\n",
    "    #for every word in review\n",
    "    \n",
    "    #split word\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    return tokens\n",
    "    #tokenized = original.tokenizeFunct column\n",
    "my_data['Tokenized'] = my_data['Cleaned_Review'].apply(lambda x:data_tokenization(x.lower()))\n",
    "\n",
    "my_data[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097dfcd2",
   "metadata": {},
   "source": [
    "## Test case sensitivity NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babcb64f",
   "metadata": {},
   "source": [
    "Purpose of using .lower method is to turn all capital letters to lower case so it knows the words are the same thing. \n",
    "\n",
    "To us, SONIC is the same as sonic, but since Python is case senitive, it see the two words as different things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36e0be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"SONIC\" == \"sonic\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0717c4a",
   "metadata": {},
   "source": [
    "## 4) Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b679ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15297261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive/Negative rating</th>\n",
       "      <th>Cleaned_Review</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>Remove_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I think food should have flavor and texture and both were lacking.</td>\n",
       "      <td>0</td>\n",
       "      <td>I think food should have flavor and texture and both were lacking</td>\n",
       "      <td>[i, think, food, should, have, flavor, and, texture, and, both, were, lacking, ]</td>\n",
       "      <td>{, texture, flavor, lacking, food, think}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "      <td>Appetite instantly gone</td>\n",
       "      <td>[appetite, instantly, gone, ]</td>\n",
       "      <td>{appetite, , gone, instantly}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Overall I was not impressed and would not go back.</td>\n",
       "      <td>0</td>\n",
       "      <td>Overall I was not impressed and would not go back</td>\n",
       "      <td>[overall, i, was, not, impressed, and, would, not, go, back, ]</td>\n",
       "      <td>{, go, would, back, impressed, overall}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>The whole experience was underwhelming, and I think we'll just go to Ninja Sushi next time.</td>\n",
       "      <td>0</td>\n",
       "      <td>The whole experience was underwhelming  and I think we ll just go to Ninja Sushi next time</td>\n",
       "      <td>[the, whole, experience, was, underwhelming, and, i, think, we, ll, just, go, to, ninja, sushi, ...</td>\n",
       "      <td>{, go, ninja, next, time, whole, sushi, experience, underwhelming, think}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>Then  as if I hadn t wasted enough of my life there  they poured salt in the wound by drawing ou...</td>\n",
       "      <td>[then, as, if, i, hadn, t, wasted, enough, of, my, life, there, they, poured, salt, in, the, wou...</td>\n",
       "      <td>{wasted, , poured, life, enough, time, bring, wound, drawing, salt, check, took}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   Review  \\\n",
       "996                                    I think food should have flavor and texture and both were lacking.   \n",
       "997                                                                              Appetite instantly gone.   \n",
       "998                                                    Overall I was not impressed and would not go back.   \n",
       "999           The whole experience was underwhelming, and I think we'll just go to Ninja Sushi next time.   \n",
       "1000  Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing ou...   \n",
       "\n",
       "     Positive/Negative rating  \\\n",
       "996                         0   \n",
       "997                         0   \n",
       "998                         0   \n",
       "999                         0   \n",
       "1000                        0   \n",
       "\n",
       "                                                                                           Cleaned_Review  \\\n",
       "996                                    I think food should have flavor and texture and both were lacking    \n",
       "997                                                                              Appetite instantly gone    \n",
       "998                                                    Overall I was not impressed and would not go back    \n",
       "999           The whole experience was underwhelming  and I think we ll just go to Ninja Sushi next time    \n",
       "1000  Then  as if I hadn t wasted enough of my life there  they poured salt in the wound by drawing ou...   \n",
       "\n",
       "                                                                                                Tokenized  \\\n",
       "996                      [i, think, food, should, have, flavor, and, texture, and, both, were, lacking, ]   \n",
       "997                                                                         [appetite, instantly, gone, ]   \n",
       "998                                        [overall, i, was, not, impressed, and, would, not, go, back, ]   \n",
       "999   [the, whole, experience, was, underwhelming, and, i, think, we, ll, just, go, to, ninja, sushi, ...   \n",
       "1000  [then, as, if, i, hadn, t, wasted, enough, of, my, life, there, they, poured, salt, in, the, wou...   \n",
       "\n",
       "                                                                           Remove_stop  \n",
       "996                                          {, texture, flavor, lacking, food, think}  \n",
       "997                                                      {appetite, , gone, instantly}  \n",
       "998                                            {, go, would, back, impressed, overall}  \n",
       "999          {, go, ninja, next, time, whole, sushi, experience, underwhelming, think}  \n",
       "1000  {wasted, , poured, life, enough, time, bring, wound, drawing, salt, check, took}  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    my_text = {word for word in tokenized_list if word not in stopword}\n",
    "    return my_text\n",
    "\n",
    "my_data['Remove_stop'] = my_data['Tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "my_data[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f638488",
   "metadata": {},
   "source": [
    "## 5) Split into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc744867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#use punctuation removed text,  review, and tokenized version to test if model can properly categorize each review\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(my_data[[\"Cleaned_Review\",\"Tokenized\",\"Remove_stop\"]], my_data[\"Positive/Negative rating\"], test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820080f3",
   "metadata": {},
   "source": [
    "## 6) Vectorize data\n",
    "\n",
    "Turn the text into numerical points for the model to easily use for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce79422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2262d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>Remove_stop</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>[you, won, t, be, disappointed, ]</td>\n",
       "      <td>{, disappointed}</td>\n",
       "      <td>0.544608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243810</td>\n",
       "      <td>0.117550</td>\n",
       "      <td>0.159184</td>\n",
       "      <td>0.098502</td>\n",
       "      <td>0.233051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>[furthermore, you, can, t, even, find, hours, of, operation, on, the, website, ]</td>\n",
       "      <td>{, furthermore, hours, website, even, find, operation}</td>\n",
       "      <td>0.690599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085880</td>\n",
       "      <td>0.289841</td>\n",
       "      <td>0.112142</td>\n",
       "      <td>0.069393</td>\n",
       "      <td>0.054727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[i, just, don, t, know, how, this, place, managed, to, served, the, blandest, food, i, have, eve...</td>\n",
       "      <td>{, managed, indian, preparing, place, ever, cuisine, blandest, know, food, eaten, served}</td>\n",
       "      <td>0.732504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204954</td>\n",
       "      <td>0.131755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110405</td>\n",
       "      <td>0.087071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091302</td>\n",
       "      <td>0.226523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>[i, would, recommend, saving, room, for, this, ]</td>\n",
       "      <td>{, saving, would, recommend, room}</td>\n",
       "      <td>0.645490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154807</td>\n",
       "      <td>0.248794</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>[, the, owners, really, really, need, to, quit, being, soooooo, cheap, let, them, wrap, my, frea...</td>\n",
       "      <td>{, quit, sandwich, one, soooooo, owners, wrap, let, really, need, freaking, two, cheap, papers}</td>\n",
       "      <td>0.661202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356789</td>\n",
       "      <td>0.161053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                               Tokenized  \\\n",
       "327                                                                    [you, won, t, be, disappointed, ]   \n",
       "423                     [furthermore, you, can, t, even, find, hours, of, operation, on, the, website, ]   \n",
       "121  [i, just, don, t, know, how, this, place, managed, to, served, the, blandest, food, i, have, eve...   \n",
       "553                                                     [i, would, recommend, saving, room, for, this, ]   \n",
       "611  [, the, owners, really, really, need, to, quit, being, soooooo, cheap, let, them, wrap, my, frea...   \n",
       "\n",
       "                                                                                         Remove_stop  \\\n",
       "327                                                                                 {, disappointed}   \n",
       "423                                           {, furthermore, hours, website, even, find, operation}   \n",
       "121        {, managed, indian, preparing, place, ever, cuisine, blandest, know, food, eaten, served}   \n",
       "553                                                               {, saving, would, recommend, room}   \n",
       "611  {, quit, sandwich, one, soooooo, owners, wrap, let, really, need, freaking, two, cheap, papers}   \n",
       "\n",
       "            0    1    2    3    4    5    6    7  ...        55        56  \\\n",
       "327  0.544608  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.243810  0.117550   \n",
       "423  0.690599  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.085880  0.289841   \n",
       "121  0.732504  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.204954  0.131755   \n",
       "553  0.645490  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.154807  0.248794   \n",
       "611  0.661202  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.356789   \n",
       "\n",
       "           57        58        59   60        61        62   63   64  \n",
       "327  0.159184  0.098502  0.233051  0.0  0.081459  0.000000  0.0  0.0  \n",
       "423  0.112142  0.069393  0.054727  0.0  0.114772  0.000000  0.0  0.0  \n",
       "121  0.000000  0.110405  0.087071  0.0  0.091302  0.226523  0.0  0.0  \n",
       "553  0.067383  0.000000  0.065767  0.0  0.137925  0.000000  0.0  0.0  \n",
       "611  0.161053  0.000000  0.000000  0.0  0.000000  0.000000  0.0  0.0  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vector = TfidfVectorizer(analyzer=remove_punctuation) #  'char', 'char_wb', 'word', or a callable function.\n",
    "data_vector_fit = data_vector.fit(X_train[\"Cleaned_Review\"])\n",
    "\n",
    "## error on parameters\n",
    "\n",
    "#fixed by putting function of text with removed punctuation into analyzer - 9/12\n",
    "\n",
    "data_train = data_vector_fit.transform(X_train[\"Cleaned_Review\"])\n",
    "data_test = data_vector_fit.transform(X_test[\"Cleaned_Review\"])\n",
    "\n",
    "#accepts list of object - connect data based on indicies \n",
    "#turn matrix to dataframe\n",
    "#put data in dataframe side by side\n",
    "X_train_vect = pd.concat([X_train[[\"Tokenized\",\"Remove_stop\"]], \n",
    "           pd.DataFrame(data_train.toarray())], axis=1)\n",
    "\n",
    "X_test_vect = pd.concat([X_test[[\"Tokenized\",\"Remove_stop\"]], \n",
    "           pd.DataFrame(data_test.toarray())], axis=1)\n",
    "\n",
    "X_train_vect.head()\n",
    "\n",
    "#some words are not recogized to be vectorized which is ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae4688",
   "metadata": {},
   "source": [
    "## 7) Final Evaluation of Model\n",
    "\n",
    "Use a classification model- Random Forrest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c1f43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time\n",
    "#checkpoint: https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training/model-selection-data-prep?u=74412268\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "731f270e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Feature names are only supported if all input features have string names, but your input has ['int', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m my_rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m155\u001b[39m, max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#n_jobs =-1 menas to run all parameters simultaneously\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m my_rf_model \u001b[38;5;241m=\u001b[39m \u001b[43mmy_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m my_y_pred \u001b[38;5;241m=\u001b[39m my_rf_model\u001b[38;5;241m.\u001b[39mpredict(X_test_vect)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#all parameters we want from running model\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 348\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:579\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    516\u001b[0m ):\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:440\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m\"\"\"Set or check the `feature_names_in_` attribute.\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m       should set `reset=False`.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset:\n\u001b[1;32m--> 440\u001b[0m     feature_names_in \u001b[38;5;241m=\u001b[39m \u001b[43m_get_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_names_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names_in_ \u001b[38;5;241m=\u001b[39m feature_names_in\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:2021\u001b[0m, in \u001b[0;36m_get_feature_names\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;66;03m# mixed type of string and non-string is not supported\u001b[39;00m\n\u001b[0;32m   2020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(types) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m types:\n\u001b[1;32m-> 2021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2022\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names are only supported if all input features have string names, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2023\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut your input has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as feature name / column name types. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2024\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want feature names to be stored and validated, you must convert \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem all to strings, by using X.columns = X.columns.astype(str) for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample. Otherwise you can remove feature / column names from your input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2027\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata, or convert them all to a non-string data type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2028\u001b[0m     )\n\u001b[0;32m   2030\u001b[0m \u001b[38;5;66;03m# Only feature names of all strings are supported\u001b[39;00m\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(types) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Feature names are only supported if all input features have string names, but your input has ['int', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type."
     ]
    }
   ],
   "source": [
    "my_rf = RandomForestClassifier(n_estimators=155, max_depth = None, n_jobs=-1)\n",
    "#n_jobs =-1 menas to run all parameters simultaneously\n",
    "\n",
    "#train model\n",
    "my_rf_model = my_rf.fit(X_train_vect, Y_train)\n",
    "my_y_pred = my_rf_model.predict(X_test_vect)\n",
    "\n",
    "#all parameters we want from running model\n",
    "precision,recall, fscore, train_support = score(Y_test,my_y_pred,pos_label=\"PositiveRev\", average=\"binary\")\n",
    "#output results - rounding 3  decimal places\n",
    "print(\"Precision: {} | ) Recall: {} / Accuracy: {}\".format(\n",
    "    round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))\n",
    "\n",
    "## need to fix X_train_vect = pd.concat([X_train[[\"Tokenized\",\"Remove_stop\"]], \n",
    "#            pd.DataFrame(data_train.toarray())], axis=1)\n",
    "\n",
    "# X_test_vect = pd.concat([X_test[[\"Tokenized\",\"Remove_stop\"]], \n",
    "#            pd.DataFrame(data_test.toarray())], axis=1)\n",
    "\n",
    "# X_train_vect.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
